{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data IO (input/output)\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "ESRF data (used to) come in (too many) different formats:\n",
    "\n",
    "* Specfile, EDF, HDF5\n",
    "* And specific detector formats: MarCCD, Pilatus CBF, Dectris Eiger, â€¦\n",
    "\n",
    "\n",
    "HDF5 is now the standard ESRF data format so we will only focus on it today.\n",
    "\n",
    "Methods for accessing other file formats are described in the [io_spec_edf.ipynb](io_spec_edf.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HDF5\n",
    "\n",
    "## What is HDF5?\n",
    "\n",
    "[HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) (for Hierarchical Data Format version 5) is a file format to structure and store complex and large volumes of data.\n",
    "\n",
    "## Structure\n",
    "\n",
    "HDF5 organizes data in a hierarchical structure, similar to a file system.\n",
    "\n",
    "It contains datasets (arrays) and groups (directories) that can hold both datasets and other groups.\n",
    "\n",
    "**Data can be mostly anything: image, table, graphs, documents**\n",
    "\n",
    "## Why HDF5?\n",
    "\n",
    "* High-performance (binary)\n",
    "* Portable file format (Standard exchange format for heterogeneous data)\n",
    "* Self-describing extensible types, rich metadata\n",
    "* Support data compression\n",
    "* Free ( & open source)\n",
    "* Widely used in scientific computing\n",
    "* Adopted by a large number of institutes (NASA, LIGO, ...)\n",
    "* Adopted by most of the synchrotrons (ESRF, SOLEIL, Desy...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HDF5 description\n",
    "\n",
    "The container is mostly structured with:\n",
    "\n",
    "* **File**: the root of the container\n",
    "* **Group**: a grouping structure containing groups or datasets\n",
    "* **Dataset**: a multidimensional array of data elements\n",
    "* And other features (links, attributes, datatypes, virtual datasets)\n",
    "\n",
    "![hdf5_class_diag](images/hdf5_model.png \"hdf5 class diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## HDF5 example\n",
    "\n",
    "Here is an example of the file generated by [pyFAI](https://github.com/silx-kit/pyFAI)\n",
    "\n",
    "![hdf5_example](images/hdf5_example.png \"hdf5 example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Useful tools for HDF5\n",
    "\n",
    "### [HDFGroup tools](https://portal.hdfgroup.org/display/HDF5/HDF5+Tools+by+Category)\n",
    "\n",
    "Command line and desktop application: `h5ls`, `h5dump`, `hdfview`\n",
    "\n",
    "```bash\n",
    ">>> h5ls -r my_first_one.h5\n",
    "    /                        Group\n",
    "    /data1                   Dataset {100, 100}\n",
    "    /group1                  Group\n",
    "    /group1/data2            Dataset {100, 100}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [`h5glance`](https://github.com/European-XFEL/h5glance)\n",
    "\n",
    "Jupyter notebook and command line tool for browsing HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# From jupyter notebook\n",
    "from h5glance import H5Glance\n",
    "H5Glance(\"data/water.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# From the command line\n",
    "h5glance data/water.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [`jupyterlab-h5web`](https://github.com/silx-kit/jupyterlab-h5web/)\n",
    "\n",
    "JupyterLab HDF5 file browser and viewer\n",
    "\n",
    "[Go to JupyterLab](/lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from jupyterlab_h5web import H5Web\n",
    "\n",
    "H5Web(\"data/water.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [`silx view`](http://www.silx.org/doc/silx/latest/applications/view.html)\n",
    "\n",
    "Desktop application file browser/viewer\n",
    "\n",
    "```bash\n",
    ">>> pip install silx\n",
    ">>> silx view my_file.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# With silx view GUI\n",
    "silx view data/water.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# h5py\n",
    "\n",
    "![h5py book](images/h5py.png \"h5py book\")\n",
    "\n",
    "## What is h5py ?\n",
    "\n",
    "[h5py](https://www.h5py.org/) is the python binding for accessing HDF5 files. Originally from [Andrew Collette](http://shop.oreilly.com/product/0636920030249.do)\n",
    "\n",
    "It allows to read / write HDF5 files using a simple Pythonic API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to install h5py ?\n",
    "\n",
    "```bash\n",
    "pip install h5py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "print(\"h5py:\", h5py.version.version)\n",
    "print(\"hdf5:\", h5py.version.hdf5_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Opening and creating HDF5 files ?\n",
    "\n",
    "### `h5py.File`\n",
    "\n",
    "* First open the file with [h5py.File](http://docs.h5py.org/en/stable/high/file.html):\n",
    "  ```\n",
    "  h5py.File('myfile.hdf5', mode)\n",
    "  ```\n",
    "  [opening modes](http://docs.h5py.org/en/stable/high/file.html#opening-creating-files):\n",
    "\n",
    "| Mode    | Meaning                                                            |\n",
    "|---------|--------------------------------------------------------------------|\n",
    "| r       | Readonly, file must exist; *Default with h5py* **v3**              |\n",
    "| r+      | Read/write, file must exist                                        |\n",
    "| w       | Create file, truncate if exists                                    |\n",
    "| w- or x | Create file, fail if exists                                        |\n",
    "| a       | Read/write if exists, create otherwise; *Default with h5py* **v2** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening an existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5file = h5py.File(\"data/water.h5\", mode=\"r\")\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5file = h5py.File(\"data/new_data.h5\", mode=\"w\")\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using a context manager\n",
    "\n",
    "* Context managers guarantee that resources are released. In our case, it ensures that the HDF5 file is closed.\n",
    "* Usually used from the `with` statement.\n",
    "\n",
    "To safely access a HDF5 file, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"data/water.h5\", mode=\"r\") as h5file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### File structure\n",
    "\n",
    "#### `h5py.Group`\n",
    "\n",
    "Documentation: [Group](http://docs.h5py.org/en/stable/high/group.html)\n",
    "\n",
    "##### browsing groups\n",
    "\n",
    "* Then access the file content with a dictionary-like API, [h5py.Group](http://docs.h5py.org/en/stable/high/group.html):\n",
    "\n",
    "  - [`Group.keys()`](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.keys)\n",
    "  - [`Group.items()`](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.items)\n",
    "  - [`Group.values()`](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Available names at the first level\n",
    "with h5py.File(\"data/water.h5\", mode=\"r\") as h5file:\n",
    "    print(list(h5file.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# List 'entry_0000' group children\n",
    "with h5py.File(\"data/water.h5\", mode=\"r\") as h5file:\n",
    "    group = h5file[\"entry_0000\"]\n",
    "    pprint(dict(group.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 'entry_0000/4_azimuthal_integration' group children\n",
    "with h5py.File(\"data/water.h5\", mode=\"r\") as h5file:\n",
    "    group = h5file[\"entry_0000\"]\n",
    "    print(list(group[\"4_azimuthal_integration\"].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 'entry_0000/4_azimuthal_integration/results' group children\n",
    "with h5py.File(\"data/water.h5\", mode=\"r\") as h5file:\n",
    "    print(list(h5file[\"/entry_0000/4_azimuthal_integration/results\"].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### creating a group\n",
    "\n",
    "With [Group.create_group](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.create_group):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 'entry_0000' group children\n",
    "with h5py.File(\"data/new_data.h5\", mode=\"w\") as h5file:\n",
    "    my_group = h5file.create_group('my_group')\n",
    "    my_group.create_group(\"sub_group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `h5py.Dataset`\n",
    "\n",
    "Documentation: [Dataset](http://docs.h5py.org/en/stable/high/dataset.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"data/water.h5\", mode=\"r\") as h5file:\n",
    "    h5dataset = h5file[\"/entry_0000/4_azimuthal_integration/results/I\"]\n",
    "    print(h5dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It mimics `numpy.ndarray`.\n",
    "The data is read from the file only when it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"data/water.h5\", mode=\"r\") as h5file:\n",
    "    h5dataset = h5file[\"/entry_0000/4_azimuthal_integration/results/I\"]\n",
    "    print(\"Dataset:\", h5dataset.shape, h5dataset.dtype, h5dataset.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from the file to a numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/water.h5\", mode=\"r\") as h5file:\n",
    "    h5dataset = h5file[\"/entry_0000/4_azimuthal_integration/results/I\"]\n",
    "    subset = h5dataset[:5]  # Copy the selection to a numpy.ndarray\n",
    "    print(\"subset:\", subset, \"=> sum:\", subset.sum())\n",
    "    \n",
    "    data = h5dataset[()]  # Copy the whole dataset to a numpy.ndarray\n",
    "    print(\"data type:\", type(data), \"; shape\", data.shape, \"; min.:\", data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once the file is closed, the Dataset no longer gives access to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"data/water.h5\", mode=\"r\") as h5file:\n",
    "    h5dataset = h5file[\"/entry_0000/4_azimuthal_integration/results/I\"]\n",
    "    subset = h5dataset[:5]  # Copy the selection to a numpy.ndarray\n",
    "    data = h5dataset[()]  # Copy the whole dataset to a numpy.ndarray\n",
    "print(h5dataset)\n",
    "print(subset)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/water.h5\", \"r\") as h5file:\n",
    "    dataset = h5file[\"/entry_0000/4_azimuthal_integration/results/I\"]\n",
    "    data = dataset[()]\n",
    "print(dataset)\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not very convenient for interactive browsing... this is why silx view, h5web, h5glance ... exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### writing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "with h5py.File(\"data/new_data.h5\", mode=\"w\") as h5file:\n",
    "    h5file[\"mydataset\"] = numpy.random.rand(100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h5py will create all missing groups to solve the dataset location. So you usually won't have to call 'create_group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"data/new_data.h5\", mode=\"a\") as h5file:\n",
    "    h5file[\"group/to/mydataset\"] = numpy.random.rand(100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative: using [Group.create_dataset](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.create_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/new_data.h5\", mode=\"w\") as h5file:\n",
    "    h5file.create_dataset(\"data1\", data=numpy.arange(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[attributes](https://docs.h5py.org/en/stable/high/attr.html) are the way to store metadata to a [group](https://docs.h5py.org/en/stable/high/group.html) or a [dataset](https://docs.h5py.org/en/stable/high/dataset.html).\n",
    "\n",
    "Group and Dataset have a small `'<obj>.attrs'` attached to them.\n",
    "\n",
    "**warning** attributes must be of a limited size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writting an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"data/new_data.h5\", \"w\") as h5file:\n",
    "    dataset = h5file.create_dataset('my_dataset', data=numpy.random.rand(10, 10))\n",
    "    dataset.attrs[\"description\"] = 'This is a random dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"data/new_data.h5\", \"r\") as h5file:\n",
    "    print(h5file[\"my_dataset\"].attrs['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercice: Flat field correction\n",
    "\n",
    "Flat-field correction is a technique used to improve quality in digital imaging.\n",
    "\n",
    "The goal is to normalize images and remove artifacts caused by variations in the pixel-to-pixel sensitivity of the detector and/or by distortions in the optical path. (see https://en.wikipedia.org/wiki/Flat-field_correction)\n",
    "\n",
    "$$ normalized = \\frac{raw - dark}{flat - dark} $$\n",
    "\n",
    "* `normalized`: Image after flat field correction\n",
    "* `raw`: Raw image. It is acquired with the sample.\n",
    "* `flat`: Flat field image. It is the response given out by the detector for a uniform input signal. This image is acquired without the sample.\n",
    "* `dark`: Also named `background` or `dark current`. It is the response given out by the detector when there is no signal. This image is acquired without the beam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a function implementing the flat field correction:\n",
    "\n",
    "*Note: make sure you execute the cell for defining this function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def flatfield_correction(raw, flat, dark):\n",
    "    \"\"\"\n",
    "    Apply a flat-field correction to a raw data using a flat and a dark.\n",
    "    \"\"\"\n",
    "    # Make sure that the computation is done using float\n",
    "    # to avoid type overflow or loss of precision\n",
    "    raw = raw.astype(numpy.float32)\n",
    "    flat = flat.astype(numpy.float32)\n",
    "    dark = dark.astype(numpy.float32)\n",
    "    # Do the computation\n",
    "    return (raw - dark) / (flat - dark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**: If you like to plot an image you can use `matplotlib`'s `imshow` function.\n",
    "\n",
    "The `%matplotlib` \"magic\" command should be called once first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "plt.imshow(numpy.random.random((20, 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "1. Browse the file ``data/ID16B_diatomee.h5``\n",
    "2. Get **a single** raw dataset, a flat field dataset and a dark image dataset from this file\n",
    "3. Apply the flat field correction\n",
    "4. Save the result into a new HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise1.py](./solutions/exercise1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from jupyterlab_h5web import H5Web\n",
    "H5Web(\"data/ID16B_diatomee.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "from h5glance import H5Glance\n",
    "H5Glance(\"data/ID16B_diatomee.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"data/ID16B_diatomee.h5\", mode=\"r\") as h5s:\n",
    "    pass\n",
    "    # this is a comment\n",
    "\n",
    "    # step1: Read the data\n",
    "\n",
    "    # raw_data_path = ...\n",
    "    # raw_data = ...\n",
    "\n",
    "    # flat_path = ...\n",
    "    # flat = ...\n",
    "\n",
    "    # dark_path = ...\n",
    "    # dark = ...\n",
    "\n",
    "# step2: Compute the result\n",
    "\n",
    "# normalized = flatfield_correction(raw_data, flat, dark)\n",
    "\n",
    "# step3: Save the result\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "1. Apply the flat field correction **to all** raw data available (use the same flat and dark for all the images)\n",
    "2. Save each result into different datasets of the same HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise2.py](./solutions/exercise2.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "From the previous exercise, we can see that the flat field correction was not very good for the last images.\n",
    "\n",
    "Another flat field was acquired at the end of the acquisition.\n",
    "\n",
    "We could use this information to compute a flat field closer to the image we want to normalize. It can be done with a linear interpolation of the flat images by using the name of the image as the interpolation factor (which varies between 0 and 500 in this case).\n",
    "\n",
    "1. For each raw data, compute the corresponding flat field using lineal interpolation (between `flatfield/0000` and `flatfield/0500`)\n",
    "2. Save each result into different datasets in a single HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise3.py](./solutions/exercise3.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset compression\n",
    "\n",
    "Install [hdf5plugin](https://github.com/silx-kit/hdf5plugin) and `import hdf5plugin`.\n",
    "\n",
    "HDF5 provides dataset compression support.\n",
    "With `h5py` GZIP and LZF compression are available (see [compression-filters](https://docs.h5py.org/en/stable/high/dataset.html#lossless-compression-filters)).\n",
    "Yet, there are many [third-party compression filters for HDF5](https://portal.hdfgroup.org/display/support/Registered+Filter+Plugins) available.\n",
    "\n",
    "[hdf5plugin](https://github.com/silx-kit/hdf5plugin) allows usage of some of those compression filters with `h5py` (Blosc, Blosc2, BitShuffle, BZip2, FciDecomp, LZ4, SZ, SZ3, Zfp, ZStd).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import hdf5plugin  # Allows to read dataset stored with supported compressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To write compressed datasets, see:\n",
    "\n",
    "- [Group.create_dataset](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.create_dataset) `chunks`, `compression` and `compression_opts` parameters.\n",
    "- [\"Chunked Storage\" documentation](https://docs.h5py.org/en/stable/high/dataset.html#chunked-storage)\n",
    "- [hdf5plugin documentation](https://github.com/silx-kit/hdf5plugin#documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Soft and external links\n",
    "\n",
    "A HDF5 file can contain links to Group/Dataset:\n",
    "- within the same file: see [h5py.SoftLink](https://docs.h5py.org/en/stable/high/group.html#soft-links)\n",
    "- in another file: see [h5py.ExternalLink](https://docs.h5py.org/en/stable/high/group.html#external-links)\n",
    "\n",
    "Links can be dangling if the destination does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### External dataset\n",
    "\n",
    "A HDF5 file can contain datasets that are stored in external binary files: See [Group.create_dataset](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.create_dataset) `external` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Virtual Dataset (aka. VDS)\n",
    "\n",
    "Virtual dataset allows to map multiple datasets into a single one.\n",
    "Once created it behaves as other datasets.\n",
    "\n",
    "See https://docs.h5py.org/en/stable/vds.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HDF5 file locking: A word of caution\n",
    "\n",
    "Do **NOT** open a HDF5 file that is otherwise being written (without caution).\n",
    "\n",
    "By default, HDF5 locks the file even for reading, and other processes cannot open it for writing.\n",
    "This can be an issue, e.g., during acquisition.\n",
    "\n",
    "**WARNING**: Without file locking, do not open twice the same file for writing or the file will be corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Workarounds:\n",
    "\n",
    "- Helper to handle HDF5 file locking: [`silx.io.h5py_utils.File`](http://www.silx.org/doc/silx/latest/modules/io/h5py_utils.html#silx.io.h5py_utils.File)\n",
    "- HDF5 file locking can be disabled by setting the `HDF5_USE_FILE_LOCKING` environment variable to `FALSE`.\n",
    "- With recent version of `h5py` (>= v3.5.0): [`h5py.File`'s `locking` argument](https://github.com/h5py/h5py/blob/f155036478ca458924d2c46edfd6bfb9e6e32fb5/h5py/_hl/files.py#L443-L451)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunked storage\n",
    "\n",
    "By default HDF5 datasets will be contiguous (like C). If you have specific usages and want to improve speed maybe you will want to define chunks. See [h5py chunked storage](https://docs.h5py.org/en/stable/high/dataset.html#chunked-storage) (for advanced usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Practical tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- conversion:\n",
    "    - [`silx convert`](http://www.silx.org/doc/silx/latest/applications/convert.html): To convert EDF, or spec files to HDF5\n",
    "\n",
    "- reading/writing HDF5 helpers:\n",
    "    - [`silx.io.dictdump`](http://www.silx.org/doc/silx/latest/modules/io/dictdump.html): `h5todict`, `dicttoh5`\n",
    "    - [`silx.io.utils.h5py_read_dataset`](http://www.silx.org/pub/doc/silx/latest/modules/io/utils.html#silx.io.utils.h5py_read_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A word about Nexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[Nexus](https://www.nexusformat.org/) is a data format for neutron, x-ray, and muon science.\n",
    "\n",
    "It aims to be a common data format for scientists for greater collaboration.\n",
    "\n",
    "If you intend to store some data to be shared it can give you a 'standard way' for storing it.\n",
    "\n",
    "The main advantage is to ensure compatibility between your data files and existing softwares (if they respect the nexus format) or from your software to different datasets.\n",
    "\n",
    "* an example on [how to store tomography raw data](http://download.nexusformat.org/doc/html/classes/applications/NXtomo.html?highlight=tomography)\n",
    "* an example to store [tomoraphy application (3D reconstruction)](http://download.nexusformat.org/doc/html/classes/applications/NXtomoproc.html?highlight=tomography)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "[h5py](https://www.h5py.org/) provides access to HDF5 file content from Python through:\n",
    "\n",
    "- [`h5py.File`](https://docs.h5py.org/en/stable/high/file.html) opens a HDF5 file:\n",
    "  - Do not forget the mode: `'r'`, `'a'`, `'w'`.\n",
    "  - Use a `with` statement or do not forget to `close` the file.\n",
    "- [`h5py.Group`](https://docs.h5py.org/en/stable/high/group.html) provides a key-value mapping `dict`-like access to the HDF5 structure.\n",
    "- [`h5py.Dataset`](https://docs.h5py.org/en/stable/high/dataset.html) gives access to data as `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
