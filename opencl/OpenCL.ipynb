{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# GPU programming with PyOpenCL\n",
    "\n",
    "Layout:\n",
    "\n",
    "1. Comparison of modern CPU and GPU\n",
    "2. Introduction to OpenCL \n",
    "3. Comparison with other ways of programming GPU\n",
    "4. My very first kernel: Element-wise addition\n",
    "5. Parallel programming design patterns\n",
    "6. Metaprogramming in PyOpenCL\n",
    "7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Comparison of modern CPU and GPU\n",
    "<div>\n",
    "    <img src=\"cpu_gpu_2024.png\" width=\"1200\">\n",
    "</div>\n",
    "They are not that different: both are many-core processors.\n",
    "\n",
    "* CPUs are still faster (MHz) and have lower latency to memory\n",
    "* GPUs are have more compute power and larger bandwidth to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction to OpenCL\n",
    "* Vendor & platform neutral parallel programming language to address CPU, GPU, FPGA and other types of accelerators. \n",
    "* Initated by Apple in 2008 and managed by the Khronos group, currently at version 3.0. \n",
    "\n",
    "* There is a strict separation between host code and device code (called kernels).\n",
    "* Kernels are written in a subset of C99 (without pointers, nor recursion) and compiled at _runtime_.\n",
    "\n",
    "* Bindings for host code, initially in C are available for all programming languages.\n",
    "\n",
    "* Sycl (Intel) is the descendent of OpenCL where C++ host code can be mixed with kernels (à la CUDA).\n",
    "\n",
    "* There is a direct mapping between CUDA kernels and OpenCL (version 1.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definition:\n",
    "\n",
    "In parallel programming, a `kernel` is the core part of the computational code, without the outer loop. It runs in parallel on all cores of the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = 1000\n",
    "a = np.random.random(size)\n",
    "b = np.random.random(size)\n",
    "res = np.empty_like(a)\n",
    "\n",
    "for idx in range(size):\n",
    "    res[idx] = a[idx] + b[idx]\n",
    "\n",
    "# ⚠ This is not vectorized numpy code ⚠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The kernel is `res[i] = a[i] + b[i]`.\n",
    "\n",
    "The size of the problem, `size`, is precised at runtime, usually in the host code.\n",
    "\n",
    "The position of the working element `idx` is obtained via specific API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU programming languages ...\n",
    "<div>\n",
    "    <img src=\"languages.svg\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyOpenCL\n",
    "\n",
    "* Expose the full OpenCL API\n",
    "* Explicit kernels in C99 \n",
    "* Explicit memory management\n",
    "* Explicit device and context management\n",
    "* Asynchronous execution with queues and events\n",
    "* Runtime compilation (since the driver contains the compiler)\n",
    "* Many metaprogramming features to expose advanced parallel algorithms\n",
    "* Integration in Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Integration with Jupyter notebooks:\n",
    "%load_ext pyopencl.ipython_ext\n",
    "\n",
    "import pyopencl as cl\n",
    "import pyopencl.array as cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose platform:\n",
      "[0] <pyopencl.Platform 'NVIDIA CUDA' at 0x55f3c21393b0>\n",
      "[1] <pyopencl.Platform 'Intel(R) OpenCL' at 0x55f3c1fead68>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choice [0]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose device(s):\n",
      "[0] <pyopencl.Device 'NVIDIA RTX A5000' on 'NVIDIA CUDA' at 0x55f3c2139420>\n",
      "[1] <pyopencl.Device 'Quadro P2200' on 'NVIDIA CUDA' at 0x55f3c21394b0>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choice, comma-separated [0]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the environment variable PYOPENCL_CTX='0:0' to avoid being asked again.\n"
     ]
    }
   ],
   "source": [
    "#Creation of the OpenCL context which holds the memory on the device\n",
    "ctx = cl.create_some_context(interactive=True)\n",
    "\n",
    "# The command queue is used to interact with the device asynchronously\n",
    "queue = cl.CommandQueue(ctx, \n",
    "                         properties=cl.command_queue_properties.PROFILING_ENABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preparation of some test data:\n",
    "\n",
    "* GPUs need to work on large chunks of data to be efficient, here we work on 64 MB of data\n",
    "* Allocation of the memory on the device and transfer of the data\n",
    "* Convention: the `_d` suffix indicates the object lives on the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the probem (16777216,)\n",
      "Allocated memory for one buffer: 67108864\n",
      "<class 'pyopencl.array.Array'> [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Array creation\n",
    "shape = (2**24,)\n",
    "print(f\"size of the probem {shape}\")\n",
    "a = np.random.random(shape).astype(np.float32)\n",
    "b = np.random.random(shape).astype(np.float32)\n",
    "print(f\"Allocated memory for one buffer: {a.nbytes}\")\n",
    "\n",
    "#Reference result, calculated in float32\n",
    "ref = a+b \n",
    "\n",
    "# Send data to the devicem using the `queue`\n",
    "a_d = cla.to_device(queue, a)\n",
    "b_d = cla.to_device(queue, b)\n",
    "#prepare output buffer\n",
    "res_d = cla.empty_like(a_d)\n",
    "\n",
    "print(type(res_d), res_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Structure of an OpenCL `kernel`:\n",
    "\n",
    "* Kernels are `void` functions: they do not return anything.\n",
    "* Kernels are declared with the `kernel` keyword\n",
    "* Arrays are declared `global` which indicates the memory of the device\n",
    "* The position of the worker is obtained with `get_global_id()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%cl_kernel\n",
    "// OpenCL kernel written in Jupyter:\n",
    "\n",
    "kernel void add(global float* a,\n",
    "                global float* b,\n",
    "                global float* res){\n",
    "    int idx = get_global_id(0);\n",
    "    res[idx] = a[idx] + b[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Integration with Jupyter:\n",
    "\n",
    "* The `%%cl_kernel` on the first line indicates the cell contains OpenCL code\n",
    "* Comments need to be indicated like in C\n",
    "* The code is compiled when the cell is executed\n",
    "* Kernels are exposed in the Python namespace automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%cl_kernel\n",
    "// Parallel programming hello-world\n",
    "\n",
    "kernel void add(global float* a,\n",
    "                global float* b,\n",
    "                global float* res){\n",
    "    int idx = get_global_id(0);\n",
    "    res[idx] = a[idx] + b[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyopencl._cl.Event object at 0x7f13681ff7d0>\n"
     ]
    }
   ],
   "source": [
    "# Execution of a kernel:\n",
    "\n",
    "evt = add(queue, shape, None,             # Indicate the queue, the size of the problem and the chunking (None=auto)\n",
    "          a_d.data, b_d.data, res_d.data) # This needs to match the signature of the kernel\n",
    "print(evt)                                # Since execution is asynchronous, the event is the handle to the processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are correct:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"The results are correct: \",\n",
    "np.allclose(ref, res_d.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time on GPU: 0.346 ms\n",
      "Execution time on CPU:\n",
      "9.29 ms ± 6.74 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Execution time on GPU: {1e-6*(evt.profile.end-evt.profile.start):.3f} ms\\nExecution time on CPU:\")\n",
    "%timeit a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel programming design patterns\n",
    "\n",
    "* Map: Element-wise operations like the `add` kernel.\n",
    "* Gather: Stencil like operation, for example convolutions.\n",
    "* Scatter: write at variable position, requires atomic operations.\n",
    "* Reduction: perform the sum for all elements in an array.\n",
    "* Scan: Perform the `cumsum`, sum of all previous elements, used in compactions.\n",
    "* Sort: Bitonic sort is one example of parallel sort.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Beside `Map`, all kernel require dozens to hundreeds of lines of code to implement the algorithm!\n",
    "\n",
    "PyOpenCL provides templates to all those algorithms making programmer's life simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generating `Map` kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are correct: True\n"
     ]
    }
   ],
   "source": [
    "from pyopencl.elementwise import ElementwiseKernel\n",
    "\n",
    "t_add = ElementwiseKernel(ctx, \n",
    "                          arguments=\"float* a, float* b, float* res\", \n",
    "                          operation=\"res[i] = a[i] + b[i]\")\n",
    "\n",
    "res_d.fill(0.0)                  # Reset the destination array\n",
    "t_add(a_d, b_d, res_d)           # Launch the kernel\n",
    "\n",
    "print(\"Results are correct:\",\n",
    "np.allclose(ref, res_d.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 109 µs, sys: 46 µs, total: 155 µs\n",
      "Wall time: 159 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyopencl.array.Array"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even more trivial:\n",
    "c_d = a_d + b_d\n",
    "\n",
    "%time a_d + b_d\n",
    "\n",
    "type(c_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reduction kernel, like the sum of all elements in an array:\n",
    "![Reduction kernel](kernel-code-sum-reduction.png)\n",
    "\n",
    "One can use `reductions` to perform the scalar product ... \n",
    "`a.b = sum(a[i]*b[i])` where `a[i]*b[i]` is the **map** operation and the sum is the **reduction**.\n",
    "\n",
    "Reduction are not limited to addition, any associative operation is usable like `*`, `min`, `max`, ... \n",
    "\n",
    "**⚠** addition is not strictly associative for floating point numbers, but that's another story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation of the result in 32-bits: False\n",
      "Validation of the result in 64-bits: True\n"
     ]
    }
   ],
   "source": [
    "# Dot product implemented as reduction kernel\n",
    "from pyopencl.reduction import ReductionKernel\n",
    "dot = ReductionKernel(ctx, \n",
    "                      dtype_out=np.float32, \n",
    "                      neutral=\"0\",\n",
    "                      reduce_expr=\"a + b\", \n",
    "                      map_expr=\"a[i] * b[i]\",\n",
    "                      arguments=\"__global float* a, __global float* b\")\n",
    "\n",
    "print(\"Validation of the result in 32-bits:\",\n",
    "np.isclose(dot(a_d, b_d).get(), np.dot(a,b)))\n",
    "\n",
    "# 32bits precision is not enough ! \n",
    "a64 = a.astype(np.float64)\n",
    "b64 = b.astype(np.float64)\n",
    "print(\"Validation of the result in 64-bits:\",\n",
    "np.isclose(dot(a_d, b_d).get(), np.dot(a64,b64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance evaluation on the GPU:\n",
      "424 µs ± 365 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "Performance evaluation on CPU (32 bits):\n",
      "4.39 ms ± 1.26 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Performance evaluation on CPU (64 bits):\n",
      "1.91 ms ± 21.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance evaluation on the GPU:\")\n",
    "%timeit dot(a_d, b_d).get()\n",
    "\n",
    "print(\"Performance evaluation on CPU (32 bits):\")\n",
    "%timeit np.dot(a,b)\n",
    "print(\"Performance evaluation on CPU (64 bits):\")\n",
    "%timeit np.dot(a64,b64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualisation of the code generated by PyOpenCL\n",
    "\n",
    "Reduction can have multiple stages, here is it implemented with 2 kernels working with workgroups of 256 threads...\n",
    "\n",
    "* Stage 2 is reducing 256 values coming from stage 1\n",
    "* Stage 1 is reducing 256² values, i.e. 64k threads are used *simulateneously*\n",
    "* Stage 0 is a simple loop over elements, each thread is reading 1k elements in a `for` loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//CL//\n",
      "    #define PCL_GROUP_SIZE 256\n",
      "    #define PCL_READ_AND_MAP(i) (pyopencl_reduction_inp[i])\n",
      "    #define PCL_REDUCE(a, b) (a + b)\n",
      "\n",
      "        #if __OPENCL_C_VERSION__ < 120\n",
      "        #pragma OPENCL EXTENSION cl_khr_fp64: enable\n",
      "        #endif\n",
      "        #define PYOPENCL_DEFINE_CDOUBLE\n",
      "\n",
      "    #include <pyopencl-complex.h>\n",
      "\n",
      "    \n",
      "\n",
      "    typedef float pcl_out_type;\n",
      "\n",
      "    __kernel void reduce_kernel_stage2(\n",
      "      __global pcl_out_type *pcl_out__base, long pcl_out__offset,\n",
      "      __global float *pyopencl_reduction_inp, __global float *a__base, long a__offset, __global float *b__base, long b__offset, \n",
      "      long pcl_start, long pcl_step, long pcl_stop,\n",
      "      unsigned int pcl_seq_count, long n)\n",
      "    {\n",
      "        __global pcl_out_type *pcl_out = (__global pcl_out_type *) (\n",
      "            (__global char *) pcl_out__base + pcl_out__offset);\n",
      "        __global float *a = (__global float *) ((__global char *) a__base + a__offset);\n",
      "__global float *b = (__global float *) ((__global char *) b__base + b__offset);\n",
      "\n",
      "        __local pcl_out_type pcl_ldata[PCL_GROUP_SIZE];\n",
      "\n",
      "        unsigned int pcl_lid = get_local_id(0);\n",
      "\n",
      "        const long pcl_base_idx =\n",
      "            get_group_id(0)*PCL_GROUP_SIZE*pcl_seq_count + pcl_lid;\n",
      "        long i = pcl_start + pcl_base_idx * pcl_step;\n",
      "\n",
      "        pcl_out_type pcl_acc = 0;\n",
      "        for (unsigned pcl_s = 0; pcl_s < pcl_seq_count; ++pcl_s)\n",
      "        {\n",
      "          if (i >= pcl_stop)\n",
      "            break;\n",
      "          pcl_acc = PCL_REDUCE(pcl_acc, PCL_READ_AND_MAP(i));\n",
      "\n",
      "          i += PCL_GROUP_SIZE*pcl_step;\n",
      "        }\n",
      "\n",
      "        pcl_ldata[pcl_lid] = pcl_acc;\n",
      "\n",
      "        \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 128)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 128]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 64)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 64]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 32)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 32]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 16)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 16]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 8)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 8]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 4)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 4]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 2)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 2]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 1)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 1]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "\n",
      "        if (pcl_lid == 0) pcl_out[get_group_id(0)] = pcl_ldata[0];\n",
      "    }\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(dot.stage_2_inf.source) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//CL//\n",
      "    #define PCL_GROUP_SIZE 256\n",
      "    #define PCL_READ_AND_MAP(i) (a[i] * b[i])\n",
      "    #define PCL_REDUCE(a, b) (a + b)\n",
      "\n",
      "        #if __OPENCL_C_VERSION__ < 120\n",
      "        #pragma OPENCL EXTENSION cl_khr_fp64: enable\n",
      "        #endif\n",
      "        #define PYOPENCL_DEFINE_CDOUBLE\n",
      "\n",
      "    #include <pyopencl-complex.h>\n",
      "\n",
      "    \n",
      "\n",
      "    typedef float pcl_out_type;\n",
      "\n",
      "    __kernel void reduce_kernel_stage1(\n",
      "      __global pcl_out_type *pcl_out__base, long pcl_out__offset,\n",
      "      __global float *a__base, long a__offset, __global float *b__base, long b__offset, \n",
      "      long pcl_start, long pcl_step, long pcl_stop,\n",
      "      unsigned int pcl_seq_count, long n)\n",
      "    {\n",
      "        __global pcl_out_type *pcl_out = (__global pcl_out_type *) (\n",
      "            (__global char *) pcl_out__base + pcl_out__offset);\n",
      "        __global float *a = (__global float *) ((__global char *) a__base + a__offset);\n",
      "__global float *b = (__global float *) ((__global char *) b__base + b__offset);\n",
      "\n",
      "        __local pcl_out_type pcl_ldata[PCL_GROUP_SIZE];\n",
      "\n",
      "        unsigned int pcl_lid = get_local_id(0);\n",
      "\n",
      "        const long pcl_base_idx =\n",
      "            get_group_id(0)*PCL_GROUP_SIZE*pcl_seq_count + pcl_lid;\n",
      "        long i = pcl_start + pcl_base_idx * pcl_step;\n",
      "\n",
      "        pcl_out_type pcl_acc = 0;\n",
      "        for (unsigned pcl_s = 0; pcl_s < pcl_seq_count; ++pcl_s)\n",
      "        {\n",
      "          if (i >= pcl_stop)\n",
      "            break;\n",
      "          pcl_acc = PCL_REDUCE(pcl_acc, PCL_READ_AND_MAP(i));\n",
      "\n",
      "          i += PCL_GROUP_SIZE*pcl_step;\n",
      "        }\n",
      "\n",
      "        pcl_ldata[pcl_lid] = pcl_acc;\n",
      "\n",
      "        \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 128)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 128]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 64)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 64]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 32)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 32]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 16)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 16]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 8)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 8]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 4)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 4]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 2)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 2]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "            barrier(CLK_LOCAL_MEM_FENCE);\n",
      "\n",
      "            \n",
      "\n",
      "            if (pcl_lid < 1)\n",
      "            {\n",
      "                pcl_ldata[pcl_lid] = PCL_REDUCE(\n",
      "                  pcl_ldata[pcl_lid],\n",
      "                  pcl_ldata[pcl_lid + 1]);\n",
      "            }\n",
      "\n",
      "            \n",
      "\n",
      "\n",
      "        if (pcl_lid == 0) pcl_out[get_group_id(0)] = pcl_ldata[0];\n",
      "    }\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(dot.stage_1_inf.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the source code: 314 lines\n"
     ]
    }
   ],
   "source": [
    "source = dot.stage_1_inf.source + \"\\n\" + dot.stage_2_inf.source\n",
    "nblines  = len(source.split('\\n'))\n",
    "print(f\"Length of the source code: {nblines} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scan kernel, sum of all previous elements from the array:\n",
    "![Scan kernel](300px-Prefix_sum_16.svg.png)\n",
    "Their typical application is *in memory* compaction or in compression algorithms. \n",
    "\n",
    "Once again, this is not limited to the `+` operation, it is valid for any associative operation.\n",
    "\n",
    "In `numpy` this is implemented in the `cumsum` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are corrects: True\n"
     ]
    }
   ],
   "source": [
    "from pyopencl.scan import GenericScanKernel\n",
    "cumsum = GenericScanKernel(ctx, \n",
    "                           np.float32,\n",
    "                           arguments=\"__global float* ary, __global float* out\",\n",
    "                           input_expr=\"ary[i]\",\n",
    "                           scan_expr=\"a+b\", \n",
    "                           neutral=\"0\",\n",
    "                           output_statement=\"out[i] = item;\")\n",
    "cumsum(a_d, res_d)\n",
    "print(\"Results are corrects:\", np.allclose(res_d.get(),np.cumsum(a64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performances of `cumsum` implemented in pyopencl:\n",
      "2.21 ms ± 3.17 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Performances of `cumsum` implemented in numpy (32 bits):\n",
      "49.3 ms ± 30.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Performances of `cumsum` implemented in numpy (64 bits):\n",
      "54.6 ms ± 33 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Performances of `cumsum` implemented in pyopencl:\")\n",
    "%timeit cumsum(a_d, res_d).wait()\n",
    "print(\"Performances of `cumsum` implemented in numpy (32 bits):\")\n",
    "%timeit np.cumsum(a)\n",
    "print(\"Performances of `cumsum` implemented in numpy (64 bits):\")\n",
    "%timeit np.cumsum(a64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`Scan` kernels are at the core of **memory compaction** but also of **compression** algorithm, as demonstrated in:\n",
    "https://doi.org/10.1107/S1600577518000607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "* PyOpenCL is an interesting Python binding for doing GPU programming:\n",
    "  - Platform independant (i.e. no vendor lock-in).\n",
    "  - Comfort of Python and Jupyter.\n",
    "  - Full control of execution and memory management.\n",
    "  - Scales to larger projects (https://github.com/silx-kit).\n",
    "  - Can also exploit many-core CPUs (without GIL issues).\n",
    "  - Great for continuous integration of GPU code using CPU drivers.\n",
    "  - Fully open source driver exists (like PortableCL).\n",
    "* Parallel programming design patterns are well documented.\n",
    "  - Knowing them allows to address performance issues with the proper tool.\n",
    "  - Most of them are already implemented into PyOpenCL/PyCuda via metaprogramming.\n",
    "* Kudos to Andreas Kloeckner, author of PyOpenCL."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
